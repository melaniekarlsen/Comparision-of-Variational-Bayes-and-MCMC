---
title: "TP2Bayesian"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r, echo=FALSE,warning=FALSE}
install.packages("MASS",repos = 'http://cran.us.r-project.org')
install.packages("abind",repos = 'http://cran.us.r-project.org')
install.packages("mnormt",repos = 'http://cran.us.r-project.org')
install.packages("LaplacesDemon",repos = 'http://cran.us.r-project.org')
install.packages("coda",repos = 'http://cran.us.r-project.org')
library(MASS)
library(abind)
library(mnormt)
library(LaplacesDemon)
library(coda)



gmllk <- function(x , Mu , Sigma , p){
    #' Log-likelihood in the Gaussian mixture model. 
    #' x: dataset: a n*d matrix for n points with d features each.
    #' Mu: a k*d matrix with k the number of components: the centers
    #' Sigma: a d*d*k array:: the convariance matrices.
    #' p: a vector of length k: the mixture weights
    #' returns:  the log-likelihood (single number)
    k <- length(p)
    if(is.vector(x)){
        x <- matrix(x, nrow=1)}
    n <- nrow(x)
    mat_dens <- vapply(1:k, function(j){
        dmnorm(x, mean = Mu[j,], varcov = Sigma[,,j], log=FALSE)
    }, FUN.VALUE = numeric(n)) ##  n rows, k columns.
    if(is.vector(mat_dens)){
        mat_dens <- matrix(mat_dens, nrow = 1)
    }
    vect_dens <-   mat_dens%*%matrix(p,ncol=1) ## vector of size n
    return(sum(log(vect_dens)))
}


gmcdf <- function(x , Mu , Sigma , p)
    #' multivariate cumulative distribution function in a GMM. 
    #' x: a single point (vector of size d)
    #' Mu, Sigma, p: see gmllk.
    #' returns: the cdf at point x. 
{
    k <- length(p)
    vect_cdf <- vapply(1:k, function(j){
        pmnorm(x, mean = Mu[j,], varcov = Sigma[,,j])
    }, FUN.VALUE = numeric(1))
    return(sum(p*vect_cdf))
}


initPar <- function(x , k){
    #' Initialisation for VB based on kmeans. 
    #'x: dataset: a n*d matrix for n points with d features each.
    #'k: number of components for the inferred mixture
    #' returns: a list with entries p, Mu, Sigma: respectively a vector of size k (weights), a k*d matrix (centers) and a d*d*k array (empirical covariance matrix)
    init <- kmeans(x = x, centers = k, iter.max = 100, nstart = 1,
                   algorithm = c("Hartigan-Wong"), trace=FALSE)
    Mu <- init$centers
    d <- ncol(x)
    Sigma <- array(dim=c(d,d,k))
    p <- rep(0,k)
    for( i in (1:k)){
        inds = which(init$cluster==i)
        n = length(inds)
        tildeX = t(t(x[inds,]) -Mu[i,])  
        sig = 1/n * t(tildeX) %*% tildeX
        Sigma[,,i] <- sig
        p[i] <-  n/nrow(x)
    }
    return(list(p = p, Mu = Mu, Sigma = Sigma ))
}


draw_sd <- function(mu , sigma)
    #' draws  an ellipsoid  around the mean of a gaussian distribution
    #' which corresponds to the density level set of the univariate
    #' 0.95 quantile.
    #' mu: vector of size d the dimension
    #' sigma: a d*d covariance matrix.
    #' returns: a 2*100 matrix containing abscissas and ordinates of
    #' the ellipsoid to be drawn. 
{
    L <-  chol(sigma)
    angles <- seq(0, 2*pi, length.out=100)
    U <- 1.64* rbind(cos(angles), sin(angles))
    X <- mu + t(L) %*% U
    return(X)
}

nanDetector <- function(X)
    #' returns TRUE if X contains NaNs
{
   # examine data frames
   if(is.data.frame(X)){ 
       return(any(unlist(sapply(X, is.nan))))
   }
   #  examine vectors, matrices, or arrays
   if(is.numeric(X)){
       return(any(is.nan(X)))
   }
   #  examine lists, including nested lists
   if(is.list(X)){
       return(any(rapply(X, is.nan)))
   }
   return(FALSE)
}

wrapper <- function(x , y , FUN, ...)
    #' applies a function on a grid with abscissas x, y.
    #' x, y: vectors of same length.
      {
       sapply(seq_along(x), FUN = function(i){FUN(x[i], y[i],...)})
      }
```

####1 - Preliminaries

## 1
```{r, echo=TRUE,warning=FALSE}
N=500
k=3
Mu=t(matrix(c(c(0,0),c(1,0),c(0,1)),nrow=2))
set.seed(1)
Sigma = rWishart(k, df=4, Sigma=0.02*diag(2))
#p = rdirichlet(1, rep(1,k))
p=c(4/10,3/10,3/10)
psi=rmultinom(N, 1, p)
X=matrix(rep(0,N*2),ncol=2)
cluster=NULL
for (i in 1:N){
  for (j in 1:k){
    if(psi[j,i]!=0){val=j}
  }
  cluster=c(cluster,val)
}
labs=cluster
for (j in 1:N){
  X[j,]=mvrnorm(1,Mu[cluster[j],],Sigma[,,cluster[j]]) }
```
## 2
```{r,echo=TRUE,warning=FALSE}
M=cbind(X, cluster)
plot(M[,1],M[,2],col=M[,3])
```

####2 - Variational Bayes
## 1
```{r}
vbMstep <- function(x, respons, alpha0 ,  W0inv , nu0 , m0 , beta0)
    #' x: the data. A n*d matrix 
    #' respons: current q(z): a n*k matrix (responsibilities r_{nk})
    #' alpha0>0: a real.  isotropic dirichlet prior parameter on p
    #' W0inv, nu0: parameters for the Wishart prior on Lambda.
    #' W0inv: d*d matrix, inverse of the Wishart parameter.
    #' nu0 > d-1:  is a real.
    #' m0 : mean parameter (d vector) for the Gaussian-Wishart prior on  mu
    #' beta0: scale parameter for the gaussian-wishart  prior on mu (>0)
    ##' 
    #' returns: a list made of ( Alpha , Winv, Nu , M , Beta):  optimal parameters for
    #' q(p),
    #' q(mu_j, Lambda_j), j=1, ...,k: 
    #' Alpha: k-vector ; Winv: d*d*k array ; Nu: a k-vector ; M: k*d matrix ;
    #' Beta: k-vector                   
{
    d <- ncol(x)
    n <- nrow(x)
    K <-  ncol(respons)
    NK <- apply(respons, 2, sum) # a vector of size k
    NK <- sapply(NK, function(x){max(x, 1e-300)}) ## avoids divisions by zero
    XK=matrix(rep(0,K*d),nrow=K)
    for (i in 1:K){XK[i,] <- (t(respons[,i])%*%x)/NK[i]}
    SK=rep(0,K)
    for (i in 1:K){
      for (j in 1:dim(x)[1]){
        SK[i]=SK[i]+respons[j,i]*((x[j,]-XK[i])%*%t(x[j,]-XK[i]))/NK[i]
      }
    }
    Alpha <- alpha0+NK

    Nu <- nu0+NK
      
    Beta <- beta0+NK
    

    M <-  (1/Beta)*(beta0*m0+NK*XK)
      ## complete the code: optimal mean parameters m_j for the mu_j's:
        ## a k*d matrix
    Winv <- array(dim=c(d,d,K))
    for( j in (1:K)){
        ## 
        ##
        Winv[,,j] <- W0inv+NK[j]*SK[j]+((beta0*NK[j])/(beta0+NK[j]))*(XK[j,]-m0)%*%(t(XK[j,]-m0))
          ## complete the code: optimal W^{-1}
            ##(inverse of the covariance parameter for Lambda_j)
            ## a d*d matrix
    }

    return(list(Alpha = Alpha, Winv = Winv, Nu = Nu, M= M, Beta =Beta)) 
}


vbEstep <- function(x, Alpha, Winv, Nu, M, Beta)
    #' computation of the variational responsibilities. 
    #' x: the data. A n*d matrix
    #' Alpha: a k vector: current dirichlet parameter for q(p)
    #' Winv : a d*d*k array: current inverses of the W parameter for the Wishart q(Lambda)
    #' Nu: a k vector: current degrees of freedom parameter for the Wishart q(Lambda)
    #' M: a k*d matrix: current mean parameters for the Gaussian q(Mu | Lambda)
    #' Beta: a k vector: current scale parameters for the Gaussian q(Mu | Lambda)
    #' returns: a n*k matrix: the responsibilities for each data point.  
{
    d <-  ncol(M)
    k <- length(Alpha)
    N <- nrow(x)
    Eloglambda <-  # k vector
        sapply(1:k, function(j){
            sum(digamma( (Nu[j] + 1 - (1:d) )/2) )+ d * log(2) - log(det(Winv[,,j]))
        })
    Elogrho <- # k vector
        digamma(Alpha) - digamma(sum(Alpha))    
    Equadratic <- # k*N  matrix
         d / Beta  + Nu * t( sapply(1:k, function(j){ ## a N * k matrix
            Wj <- solve(Winv[,,j])
            sapply(1:N, function(n){# a N vector
                t(M[j,] -x[n, ]) %*% Wj %*% (M[j,] -x[n, ])})
        }))
    logResponsT <- Elogrho+(1/2)*(Eloglambda)-(d/2)*log(2*pi)-(1/2)*Equadratic
    logRespons <- t(logResponsT) ## N * k
    logRespons <- logRespons - apply(logRespons, 1, max) #' avoids numerical precision loss. 
    respons <- exp(logRespons) ##  N * k matrix
    Z <-  apply(respons, 1 , sum ) # N vector
    respons <-  respons / Z ##N * k matrix
    return(respons)
}



vbalgo <- function(x, k, alpha0,  W0inv, nu0, m0, beta0, tol=1e-5)
    #' x: the data. n*d matrix
    #' k: the number of mixture components. 
    #' alpha0, W0inv, nu0, m0, beta0: prior hyper-parameters, see vbMstep.
    #' returns: a list composed of (Alphamat,  Winvarray, Numat, Marray, Betamat, responsarray, stopCriteria):
    #'   optimal parameters for q(p), q(mu_j, Lambda_j), j=1, ...,k, and trace of the
    ## stopping criteria along the iteration. 
    #'   Alphamat: K* Tmatrix,  Winvarray: d*d*T array,  Numat: a k*T matrix-vector, 
    #'   Marray: k*d*T array,  Betamat: k*T matrix, responsarray: n*k*T matrix, 
    #'   where T is the number of steps.
    #' stoppingCriteria: a T-vector:the stopping criterion  at each iteration (the first entry is set to the arbitrary 0 value)
{
    N <- nrow(x)
    init <-  initPar(x=x,k=k)
    d <- ncol(x)
    res <- list(Alphamat=matrix(nrow=k, ncol=0),
                Winvarray = array(dim=c(d,d,k,0)),
                Numat = matrix(nrow=k, ncol=0),
                Marray= array(dim=c(k,d,0) ),
                Betamat = matrix(nrow=k, ncol=0),
                responsarray = array(dim=c(N,k, 0)),
                stopCriteria = c(0)
                )
    Winvstart <- array(dim=c(d,d,k))
    for(j in 1:k){
        Winvstart[,,j] <- init$p[j] * N *  init$Sigma[,,j]
        }
    current <- list( Alpha = N * init$p, 
                    Winv = Winvstart, 
                    Nu = N* init$p, 
                    M = init$Mu,
                    Beta = N * init$p)
    ## current: current list of hyper parameters for the variational distribution
    
    continue <- TRUE
    niter <- 0
    while(continue){        
        niter <- niter+1
        respons <- vbEstep(x, current$Alpha, current$Winv, current$Nu, current$M, current$Beta)
        oldvec <- current
        if(nanDetector(respons)) {stop("NaNs detected!\n")}
        vbOpt <- vbMstep(x, respons, alpha0 ,  W0inv , nu0 , m0 , beta0)
            
        if(nanDetector(vbOpt)) {stop("NaNs detected!\n")}
        current <- vbOpt

        if(niter >=2){
            delta <-  norm(unlist(current, use.names=FALSE)-unlist(oldvec, use.names=FALSE),type="2") 
            res$stopCriteria <- c(res$stopCriteria,delta)
        }
        

        
        res$Alphamat <- cbind(res$Alphamat, current$Alpha)
        res$Winvarray <- abind(res$Winvarray, current$Winv,along=4)
        res$Numat <- cbind(res$Numat, current$Nu)
        res$Marray <- abind(res$Marray, current$M,along=3)
        res$Betamat <- cbind(res$Betamat, current$Beta)
        res$responsarray <- abind(res$responsarray, respons,along=3)
        
            if(niter>=2){
                if( niter == 200  ||  delta < tol)              
            {continue <- FALSE}
        }
    }
        return(res)
        
}
```

## 2
```{r, warning=FALSE}
d=2
alpha0 <- 0.1
m0 <- rep(0,2)
beta0 <- 0.1
W0 <- 1*diag(2)
nu0 <- 10
Kfit=3
#' Run VB 
#'
seed <- 10
set.seed(seed)
outputvb <- vbalgo(x=X,k=Kfit, alpha0 = alpha0, W0inv = solve(W0),
                 nu0 = nu0, m0 = m0, beta0=beta0, tol=1e-6)

#' plot the Stopping criteria over iterations
plot(outputvb$stopCriteria)

#' show a summary of VB's output
T <- ncol(outputvb$Alphamat)
outputvb$Alphamat[,T]
outputvb$Marray[,,T]
```
## 3
```{r}
p_vb <- outputvb$Alphamat[,T]/sum(outputvb$Alphamat)

Mu_vb <- outputvb$Marray[,,T]

Sigma_vb <- array(dim=c(d,d,Kfit))
for(j in 1:Kfit){

    Sigma_vb[,,j] <- solve(outputvb$Numat[j,T]*solve(outputvb$Winvarray[,,j,T]))
   
}


## show the data, true centers and initial positions from K-means
plot(X[,1], X[,2], col=labs)
points(Mu[,1],Mu[,2], col="black",pch=8,cex=10*p) 
set.seed(seed)
Init <-  initPar(X,Kfit)
points(Init$Mu[,1],Init$Mu[,2], col="orange",pch=18,cex = 10*Init$p)
## Add a  summary of the VB solution
nonneg <- which(p_vb>0.001)
for(j in nonneg){
    points(Mu_vb[j,1], Mu_vb[j,2], col="blue",
           pch=18,cex= 10 * p)
    ellips <- draw_sd(mu = Mu_vb[j,], 
                      sigma = Sigma_vb[,,j])
    lines(ellips[1,], ellips[2,], col='blue')
}    
```

## 4
```{r,warning=FALSE,echo=FALSE}
alpha0 = 0.1
m0 = rep(0,2)
beta0 = 0.1
W0 = 1*diag(2)
nu0 = 10
Kfit=3
d=ncol(X)
```
```{r,warning=FALSE,echo=FALSE}
alpha0 = 8000
print('alpha0=800')
seed <- 10
set.seed(seed)
outputvb <- vbalgo(x=X,k=Kfit, alpha0 = alpha0, W0inv = solve(W0),
                 nu0 = nu0, m0 = m0, beta0=beta0, tol=1e-6)

#' plot the Stopping criteria over iterations
plot(outputvb$stopCriteria)
```

```{r,warning=FALSE,echo=FALSE}
#' show a summary of VB's output
T <- ncol(outputvb$Alphamat)
outputvb$Alphamat[,T]
outputvb$Marray[,,T]
p_vb <- outputvb$Alphamat[,T]/sum(outputvb$Alphamat)

Mu_vb <- outputvb$Marray[,,T]

Sigma_vb <- array(dim=c(d,d,Kfit))
for(j in 1:Kfit){

    Sigma_vb[,,j] <- solve(outputvb$Numat[j,T]*solve(outputvb$Winvarray[,,j,T]))
   
}
## show the data, true centers and initial positions from K-means
plot(X[,1], X[,2], col=labs)
points(Mu[,1],Mu[,2], col="black",pch=8,cex=10*p) 
set.seed(seed)
Init <-  initPar(X,Kfit)
points(Init$Mu[,1],Init$Mu[,2], col="orange",pch=18,cex = 10*Init$p)
## Add a  summary of the VB solution
nonneg <- which(p_vb>0.001)
for(j in nonneg){
    points(Mu_vb[j,1], Mu_vb[j,2], col="blue",
           pch=18,cex= 10 * p)
    ellips <- draw_sd(mu = Mu_vb[j,], 
                      sigma = Sigma_vb[,,j])
    lines(ellips[1,], ellips[2,], col='blue')
} 
```

```{r,warning=FALSE,echo=FALSE}
alpha0=0.1
print('alpha0=0.1')
seed <- 10
set.seed(seed)
outputvb <- vbalgo(x=X,k=Kfit, alpha0 = alpha0, W0inv = solve(W0),
                 nu0 = nu0, m0 = m0, beta0=beta0, tol=1e-6)

#' plot the Stopping criteria over iterations
plot(outputvb$stopCriteria)
```

```{r,warning=FALSE,echo=FALSE}
#' show a summary of VB's output
T <- ncol(outputvb$Alphamat)
outputvb$Alphamat[,T]
outputvb$Marray[,,T]
p_vb <- outputvb$Alphamat[,T]/sum(outputvb$Alphamat)

Mu_vb <- outputvb$Marray[,,T]

Sigma_vb <- array(dim=c(d,d,Kfit))
for(j in 1:Kfit){

    Sigma_vb[,,j] <- solve(outputvb$Numat[j,T]*solve(outputvb$Winvarray[,,j,T]))
   
}


## show the data, true centers and initial positions from K-means
plot(X[,1], X[,2], col=labs)
points(Mu[,1],Mu[,2], col="black",pch=8,cex=10*p) 
set.seed(seed)
Init <-  initPar(X,Kfit)
points(Init$Mu[,1],Init$Mu[,2], col="orange",pch=18,cex = 10*Init$p)
## Add a  summary of the VB solution
nonneg <- which(p_vb>0.001)
for(j in nonneg){
    points(Mu_vb[j,1], Mu_vb[j,2], col="blue",
           pch=18,cex= 10 * p)
    ellips <- draw_sd(mu = Mu_vb[j,], 
                      sigma = Sigma_vb[,,j])
    lines(ellips[1,], ellips[2,], col='blue')
} 
```
It seems like the Variational Bayes algorithm is so good that a drastic change in the value of alpha0 doesn't influence the convergence of the algorithm.

## 5

# Kfit
```{r,warning=FALSE,echo=FALSE}
Kfit=10
print('Kfit=10')
seed <- 10
set.seed(seed)
outputvb <- vbalgo(x=X,k=Kfit, alpha0 = alpha0, W0inv = solve(W0),
                 nu0 = nu0, m0 = m0, beta0=beta0, tol=1e-6)

#' plot the Stopping criteria over iterations
plot(outputvb$stopCriteria)

#' show a summary of VB's output
T <- ncol(outputvb$Alphamat)
outputvb$Alphamat[,T]
outputvb$Marray[,,T]
p_vb <- outputvb$Alphamat[,T]/sum(outputvb$Alphamat)

Mu_vb <- outputvb$Marray[,,T]

Sigma_vb <- array(dim=c(d,d,Kfit))
for(j in 1:Kfit){

    Sigma_vb[,,j] <- solve(outputvb$Numat[j,T]*solve(outputvb$Winvarray[,,j,T]))
   
}


## show the data, true centers and initial positions from K-means
plot(X[,1], X[,2], col=labs)
points(Mu[,1],Mu[,2], col="black",pch=8,cex=10*p) 
set.seed(seed)
Init <-  initPar(X,Kfit)
points(Init$Mu[,1],Init$Mu[,2], col="orange",pch=18,cex = 10*Init$p)
## Add a  summary of the VB solution
nonneg <- which(p_vb>0.001)
for(j in nonneg){
    points(Mu_vb[j,1], Mu_vb[j,2], col="blue",
           pch=18,cex= 10 * p)
    ellips <- draw_sd(mu = Mu_vb[j,], 
                      sigma = Sigma_vb[,,j])
    lines(ellips[1,], ellips[2,], col='blue')
} 
Kfit=3
print('Kfit=3')
seed <- 10
set.seed(seed)
outputvb <- vbalgo(x=X,k=Kfit, alpha0 = alpha0, W0inv = solve(W0),
                 nu0 = nu0, m0 = m0, beta0=beta0, tol=1e-6)

#' plot the Stopping criteria over iterations
plot(outputvb$stopCriteria)

#' show a summary of VB's output
T <- ncol(outputvb$Alphamat)
outputvb$Alphamat[,T]
outputvb$Marray[,,T]
p_vb <- outputvb$Alphamat[,T]/sum(outputvb$Alphamat)

Mu_vb <- outputvb$Marray[,,T]

Sigma_vb <- array(dim=c(d,d,Kfit))
for(j in 1:Kfit){

    Sigma_vb[,,j] <- solve(outputvb$Numat[j,T]*solve(outputvb$Winvarray[,,j,T]))
   
}


## show the data, true centers and initial positions from K-means
plot(X[,1], X[,2], col=labs)
points(Mu[,1],Mu[,2], col="black",pch=8,cex=10*p) 
set.seed(seed)
Init <-  initPar(X,Kfit)
points(Init$Mu[,1],Init$Mu[,2], col="orange",pch=18,cex = 10*Init$p)
## Add a  summary of the VB solution
nonneg <- which(p_vb>0.001)
for(j in nonneg){
    points(Mu_vb[j,1], Mu_vb[j,2], col="blue",
           pch=18,cex= 10 * p)
    ellips <- draw_sd(mu = Mu_vb[j,], 
                      sigma = Sigma_vb[,,j])
    lines(ellips[1,], ellips[2,], col='blue')
} 
```

The ellipses are not visible with such a high value of Kfit.

# m0
```{r,warning=FALSE,echo=FALSE}
m0 = rep(500,2)
print('m0=rep(500,2)')
seed <- 10
set.seed(seed)
outputvb <- vbalgo(x=X,k=Kfit, alpha0 = alpha0, W0inv = solve(W0),
                 nu0 = nu0, m0 = m0, beta0=beta0, tol=1e-6)

#' plot the Stopping criteria over iterations
plot(outputvb$stopCriteria)

#' show a summary of VB's output
T <- ncol(outputvb$Alphamat)
outputvb$Alphamat[,T]
outputvb$Marray[,,T]
p_vb <- outputvb$Alphamat[,T]/sum(outputvb$Alphamat)

Mu_vb <- outputvb$Marray[,,T]

Sigma_vb <- array(dim=c(d,d,Kfit))
for(j in 1:Kfit){

    Sigma_vb[,,j] <- solve(outputvb$Numat[j,T]*solve(outputvb$Winvarray[,,j,T]))
   
}


## show the data, true centers and initial positions from K-means
plot(X[,1], X[,2], col=labs)
points(Mu[,1],Mu[,2], col="black",pch=8,cex=10*p) 
set.seed(seed)
Init <-  initPar(X,Kfit)
points(Init$Mu[,1],Init$Mu[,2], col="orange",pch=18,cex = 10*Init$p)
## Add a  summary of the VB solution
nonneg <- which(p_vb>0.001)
for(j in nonneg){
    points(Mu_vb[j,1], Mu_vb[j,2], col="blue",
           pch=18,cex= 10 * p)
    ellips <- draw_sd(mu = Mu_vb[j,], 
                      sigma = Sigma_vb[,,j])
    lines(ellips[1,], ellips[2,], col='blue')
} 
m0=rep(0,2)
print('m0=rep(0,2)')
seed <- 10
set.seed(seed)
outputvb <- vbalgo(x=X,k=Kfit, alpha0 = alpha0, W0inv = solve(W0),
                 nu0 = nu0, m0 = m0, beta0=beta0, tol=1e-6)

#' plot the Stopping criteria over iterations
plot(outputvb$stopCriteria)

#' show a summary of VB's output
T <- ncol(outputvb$Alphamat)
outputvb$Alphamat[,T]
outputvb$Marray[,,T]
p_vb <- outputvb$Alphamat[,T]/sum(outputvb$Alphamat)

Mu_vb <- outputvb$Marray[,,T]

Sigma_vb <- array(dim=c(d,d,Kfit))
for(j in 1:Kfit){

    Sigma_vb[,,j] <- solve(outputvb$Numat[j,T]*solve(outputvb$Winvarray[,,j,T]))
   
}


## show the data, true centers and initial positions from K-means
plot(X[,1], X[,2], col=labs)
points(Mu[,1],Mu[,2], col="black",pch=8,cex=10*p) 
set.seed(seed)
Init <-  initPar(X,Kfit)
points(Init$Mu[,1],Init$Mu[,2], col="orange",pch=18,cex = 10*Init$p)
## Add a  summary of the VB solution
nonneg <- which(p_vb>0.001)
for(j in nonneg){
    points(Mu_vb[j,1], Mu_vb[j,2], col="blue",
           pch=18,cex= 10 * p)
    ellips <- draw_sd(mu = Mu_vb[j,], 
                      sigma = Sigma_vb[,,j])
    lines(ellips[1,], ellips[2,], col='blue')
} 
```

The effect of the value of m0 on the prior's mean is clear from this plot. Moreover, it influences the width of the ellipse too.

# beta0
```{r,warning=FALSE,echo=FALSE}
beta0=4000
print('beta0=4000')
seed <- 10
set.seed(seed)
outputvb <- vbalgo(x=X,k=Kfit, alpha0 = alpha0, W0inv = solve(W0),
                 nu0 = nu0, m0 = m0, beta0=beta0, tol=1e-6)

#' plot the Stopping criteria over iterations
plot(outputvb$stopCriteria)

#' show a summary of VB's output
T <- ncol(outputvb$Alphamat)
outputvb$Alphamat[,T]
outputvb$Marray[,,T]
p_vb <- outputvb$Alphamat[,T]/sum(outputvb$Alphamat)

Mu_vb <- outputvb$Marray[,,T]

Sigma_vb <- array(dim=c(d,d,Kfit))
for(j in 1:Kfit){

    Sigma_vb[,,j] <- solve(outputvb$Numat[j,T]*solve(outputvb$Winvarray[,,j,T]))
   
}


## show the data, true centers and initial positions from K-means
plot(X[,1], X[,2], col=labs)
points(Mu[,1],Mu[,2], col="black",pch=8,cex=10*p) 
set.seed(seed)
Init <-  initPar(X,Kfit)
points(Init$Mu[,1],Init$Mu[,2], col="orange",pch=18,cex = 10*Init$p)
## Add a  summary of the VB solution
nonneg <- which(p_vb>0.001)
for(j in nonneg){
    points(Mu_vb[j,1], Mu_vb[j,2], col="blue",
           pch=18,cex= 10 * p)
    ellips <- draw_sd(mu = Mu_vb[j,], 
                      sigma = Sigma_vb[,,j])
    lines(ellips[1,], ellips[2,], col='blue')
} 
beta0=0.1
print('beta0=0.1')
seed <- 10
set.seed(seed)
outputvb <- vbalgo(x=X,k=Kfit, alpha0 = alpha0, W0inv = solve(W0),
                 nu0 = nu0, m0 = m0, beta0=beta0, tol=1e-6)

#' plot the Stopping criteria over iterations
plot(outputvb$stopCriteria)

#' show a summary of VB's output
T <- ncol(outputvb$Alphamat)
outputvb$Alphamat[,T]
outputvb$Marray[,,T]
p_vb <- outputvb$Alphamat[,T]/sum(outputvb$Alphamat)

Mu_vb <- outputvb$Marray[,,T]

Sigma_vb <- array(dim=c(d,d,Kfit))
for(j in 1:Kfit){

    Sigma_vb[,,j] <- solve(outputvb$Numat[j,T]*solve(outputvb$Winvarray[,,j,T]))
   
}


## show the data, true centers and initial positions from K-means
plot(X[,1], X[,2], col=labs)
points(Mu[,1],Mu[,2], col="black",pch=8,cex=10*p) 
set.seed(seed)
Init <-  initPar(X,Kfit)
points(Init$Mu[,1],Init$Mu[,2], col="orange",pch=18,cex = 10*Init$p)
## Add a  summary of the VB solution
nonneg <- which(p_vb>0.001)
for(j in nonneg){
    points(Mu_vb[j,1], Mu_vb[j,2], col="blue",
           pch=18,cex= 10 * p)
    ellips <- draw_sd(mu = Mu_vb[j,], 
                      sigma = Sigma_vb[,,j])
    lines(ellips[1,], ellips[2,], col='blue')
} 
```

Again, the effect of the value of beta0 on the prior's covariance is clear from this plot. Moreover, with a high value of beta0, the algorithm converges to a solution where two of the three Gaussians are similarly centered but have different covariance matrices. 

# W0

```{r,warning=FALSE,echo=FALSE}
W0 = 5000*diag(2)
print('W0=5000*diag(2)')
seed <- 10
set.seed(seed)
outputvb <- vbalgo(x=X,k=Kfit, alpha0 = alpha0, W0inv = solve(W0),
                 nu0 = nu0, m0 = m0, beta0=beta0, tol=1e-6)

#' plot the Stopping criteria over iterations
plot(outputvb$stopCriteria)

#' show a summary of VB's output
T <- ncol(outputvb$Alphamat)
outputvb$Alphamat[,T]
outputvb$Marray[,,T]
p_vb <- outputvb$Alphamat[,T]/sum(outputvb$Alphamat)

Mu_vb <- outputvb$Marray[,,T]

Sigma_vb <- array(dim=c(d,d,Kfit))
for(j in 1:Kfit){

    Sigma_vb[,,j] <- solve(outputvb$Numat[j,T]*solve(outputvb$Winvarray[,,j,T]))
   
}


## show the data, true centers and initial positions from K-means
plot(X[,1], X[,2], col=labs)
points(Mu[,1],Mu[,2], col="black",pch=8,cex=10*p) 
set.seed(seed)
Init <-  initPar(X,Kfit)
points(Init$Mu[,1],Init$Mu[,2], col="orange",pch=18,cex = 10*Init$p)
## Add a  summary of the VB solution
nonneg <- which(p_vb>0.001)
for(j in nonneg){
    points(Mu_vb[j,1], Mu_vb[j,2], col="blue",
           pch=18,cex= 10 * p)
    ellips <- draw_sd(mu = Mu_vb[j,], 
                      sigma = Sigma_vb[,,j])
    lines(ellips[1,], ellips[2,], col='blue')
} 
W0=diag(2)
print('W0=diag(2)')
seed <- 10
set.seed(seed)
outputvb <- vbalgo(x=X,k=Kfit, alpha0 = alpha0, W0inv = solve(W0),
                 nu0 = nu0, m0 = m0, beta0=beta0, tol=1e-6)

#' plot the Stopping criteria over iterations
plot(outputvb$stopCriteria)

#' show a summary of VB's output
T <- ncol(outputvb$Alphamat)
outputvb$Alphamat[,T]
outputvb$Marray[,,T]
p_vb <- outputvb$Alphamat[,T]/sum(outputvb$Alphamat)

Mu_vb <- outputvb$Marray[,,T]

Sigma_vb <- array(dim=c(d,d,Kfit))
for(j in 1:Kfit){

    Sigma_vb[,,j] <- solve(outputvb$Numat[j,T]*solve(outputvb$Winvarray[,,j,T]))
   
}


## show the data, true centers and initial positions from K-means
plot(X[,1], X[,2], col=labs)
points(Mu[,1],Mu[,2], col="black",pch=8,cex=10*p) 
set.seed(seed)
Init <-  initPar(X,Kfit)
points(Init$Mu[,1],Init$Mu[,2], col="orange",pch=18,cex = 10*Init$p)
## Add a  summary of the VB solution
nonneg <- which(p_vb>0.001)
for(j in nonneg){
    points(Mu_vb[j,1], Mu_vb[j,2], col="blue",
           pch=18,cex= 10 * p)
    ellips <- draw_sd(mu = Mu_vb[j,], 
                      sigma = Sigma_vb[,,j])
    lines(ellips[1,], ellips[2,], col='blue')
} 
```

Same as above. We notice that the bigger the initial variance between the data, the thinnest the posterior ellipse gets, which makes sense statistically.

# nu0

```{r,warning=FALSE,echo=FALSE}
nu0=5000
print('nu0=5000')
seed <- 10
set.seed(seed)
outputvb <- vbalgo(x=X,k=Kfit, alpha0 = alpha0, W0inv = solve(W0),
                 nu0 = nu0, m0 = m0, beta0=beta0, tol=1e-6)

#' plot the Stopping criteria over iterations
plot(outputvb$stopCriteria)

#' show a summary of VB's output
T <- ncol(outputvb$Alphamat)
outputvb$Alphamat[,T]
outputvb$Marray[,,T]
p_vb <- outputvb$Alphamat[,T]/sum(outputvb$Alphamat)

Mu_vb <- outputvb$Marray[,,T]

Sigma_vb <- array(dim=c(d,d,Kfit))
for(j in 1:Kfit){

    Sigma_vb[,,j] <- solve(outputvb$Numat[j,T]*solve(outputvb$Winvarray[,,j,T]))
   
}


## show the data, true centers and initial positions from K-means
plot(X[,1], X[,2], col=labs)
points(Mu[,1],Mu[,2], col="black",pch=8,cex=10*p) 
set.seed(seed)
Init <-  initPar(X,Kfit)
points(Init$Mu[,1],Init$Mu[,2], col="orange",pch=18,cex = 10*Init$p)
## Add a  summary of the VB solution
nonneg <- which(p_vb>0.001)
for(j in nonneg){
    points(Mu_vb[j,1], Mu_vb[j,2], col="blue",
           pch=18,cex= 10 * p)
    ellips <- draw_sd(mu = Mu_vb[j,], 
                      sigma = Sigma_vb[,,j])
    lines(ellips[1,], ellips[2,], col='blue')
} 
nu0=10
print('nu0=10')
seed <- 10
set.seed(seed)
outputvb <- vbalgo(x=X,k=Kfit, alpha0 = alpha0, W0inv = solve(W0),
                 nu0 = nu0, m0 = m0, beta0=beta0, tol=1e-6)

#' plot the Stopping criteria over iterations
plot(outputvb$stopCriteria)

#' show a summary of VB's output
T <- ncol(outputvb$Alphamat)
outputvb$Alphamat[,T]
outputvb$Marray[,,T]
p_vb <- outputvb$Alphamat[,T]/sum(outputvb$Alphamat)

Mu_vb <- outputvb$Marray[,,T]

Sigma_vb <- array(dim=c(d,d,Kfit))
for(j in 1:Kfit){

    Sigma_vb[,,j] <- solve(outputvb$Numat[j,T]*solve(outputvb$Winvarray[,,j,T]))
   
}


## show the data, true centers and initial positions from K-means
plot(X[,1], X[,2], col=labs)
points(Mu[,1],Mu[,2], col="black",pch=8,cex=10*p) 
set.seed(seed)
Init <-  initPar(X,Kfit)
points(Init$Mu[,1],Init$Mu[,2], col="orange",pch=18,cex = 10*Init$p)
## Add a  summary of the VB solution
nonneg <- which(p_vb>0.001)
for(j in nonneg){
    points(Mu_vb[j,1], Mu_vb[j,2], col="blue",
           pch=18,cex= 10 * p)
    ellips <- draw_sd(mu = Mu_vb[j,], 
                      sigma = Sigma_vb[,,j])
    lines(ellips[1,], ellips[2,], col='blue')
} 
```


####3 - Metropolis-Hastings algorithm

## vbPredictiveCdf
```{r}
vbPredictiveCdf <- function(x, Alpha, Beta, M, Winv, Nu)
    #' predictive cumulative distribution function based on the VB approximation.
    #' x: a single point (vector): where to evaluate the cdf. 
    #' Alpha, Winv, Nu, M, Beta: the VB posterior parameters,  see vbEstep
    #' returns the value of the variational posterior predictive cdf
    #'  (= mean of the mixture cdf under the variational posterior predictive)
    #'   at point x. 
{
    k <- length(Alpha)
    d <-  length(x)
    vectcdf <- vapply(X= 1:k, FUN= function(j){
        W <- Winv[,,j]
        L <-  (1 + Beta[j])/ ((Nu[j] + 1 - d) * Beta[j])  *   W
        L <- 1/2 * (L + t(L)) ## ensures symmetry despite numerical errors
        return(pmt(x=x, mean = M[j,], S = L, df = Nu[j] + 1 - d, log=FALSE))} ,
        FUN.VALUE= numeric(1) )

    return(sum(Alpha * vectcdf) / sum(Alpha))
}
```
```{r,echo=FALSE}
dprior <- function( Mu, Sigma, p,
                   hpar = list( alpha0= 1,
                               m0 = rep(0, ncol(Mu)), beta0 = 1, 
                               W0 = diag(ncol(Mu)), nu0 = ncol(Mu)))
    #'log-prior density on (Mu, Sigma, p)
    #' Mu, Sigma, p: see gmllk
    #' hpar: a list of hyper-parameters composed of
    #' - alpha0> 0 : isotropic dirichlet prior on p
    #' - m0: a d vector: mean parameter for the Gaussian-Wishart prior on Mu
    #' - beta0: a single number >0: scale parameter for the Gaussian-Wishart prior on Mu
    #' - W0: covariance parameter for the inverse-wishart distribution on Sigma
    #' - nu0: degrees of freedom >d-1 for the wishart distribution on Sigma.  
    
{
    d <- ncol(Mu)
    k <- length(p)
    prior_p <- ddirichlet(p, alpha= rep(hpar$alpha0, k), log = TRUE)
    prior_MuSigma <- sum(sapply(1:k, function(j){
        dnorminvwishart(mu = Mu[j,], mu0 = hpar$m0, lambda = hpar$beta0,
                        Sigma = Sigma[,,j], S = hpar$W0, nu = hpar$nu0,
                        log = TRUE)}))
    return(prior_p + prior_MuSigma)
}
```

## 1
```{r}
rproposal <- function( Mu, Sigma, p, ppar=list(var_Mu = 0.1,
                                               nu_Sigma = 10,
                                               alpha_p = 10))
    #' random generator according to a proposal kernel centered at the current value.
    #' Mu, Sigma, p: current mixture parameters, see gmllk.
    #' ppar: a list made of :
    #' - var_Mu: variance parameter for the gaussian kernel for Mu.
    #' - nu_Sigma: degrees of freedom for the Wihart kernel for Sigma
    #' - alpha_p: concentration aprameter for the Dirichlet kernel for p
    #' returns: a list fo proposal parameters: (Mu, Sigma, p), where 
    #'   p ~ dirichlet(Alpha) with mean = Alpha/sum(Alpha) = current p and
    #'   concentration parameter sum(Alpha) = alpha_p.
    #' Mu : a k*d matrix and Sigma: a d*d*k array: 
    #'   for j in 1:k,  Mu[j,]~ Normal(mean= current Mu[j,], covariance = var_Mu*Identity)
    #'   Sigma[,,j]~ Wishart(W = 1/nu_Sigma * current Sigma[,,j] ; nu = nu_Sigma)
    
{
    d <- ncol(Mu)
    k <- length(p)
    alphaProp <- sapply(ppar$alpha_p * p, function(x){max(x,1e-30)})
    ## this avoids numerical errors

    p <- rdirichlet(n=1, alpha = alphaProp)
    p <- sapply(p, function(x){max(x,1e-30)})
    p <- p/sum(p)
    for(j in (1:k))
    {
        Mu[j,] <- rmvn(n=1,mu=Mu[j,], Sigma = ppar$var_Mu*diag(dim(as.array(Mu[j,])))) 
        Sigma[,,j] <-  rwishart(nu = ppar$nu_Sigma,S = 1/ppar$nu_Sigma * Sigma[,,j])
    }
    return(list(Mu = Mu, Sigma = Sigma, p = p))
}
```
## 2 
```{r,warning=FALSE}
MHsample <- function(x, k, nsample,
                     init=list(Mu = matrix(0,ncol=ncol(x), nrow=k ),
                               Sigma = array(rep(diag(ncol(x)), k),
                                             dim=c(ncol(x), ncol(x), k)),
                               p = rep(1/k, k)),
                     hpar= list( alpha0= 1, 
                                m0 = rep(0, ncol(Mu)), beta0 = 1, 
                                W0 = diag(ncol(Mu)), nu0 = ncol(Mu)),
                     ppar = list(var_Mu = 0.1,
                                 nu_Sigma = 10,
                                 alpha_p = 10) )
    #' x: the data. A n*d matrix.
    #' k: the number of mixture components.
    #' nsample: number of MCMC iterations
    #' init: starting value for the the MCMC. Format: list(Mu, Sigma, p), see gmllk for details
    #' hpar: a list of hyper-parameter for the prior: see dprior.
    #' ppar: a list of parameter for the proposal: see rproposal.
    #' returns: a sample produced by the Metropolis-Hastings algorithm, together with
    #' the log-posterior density (unnormalized) across iterations, and number of acepted proposals.  as a list composed of
    #' - p: a k*nsample matrix
    #' - Mu: a k*d*nsample array
    #' - Sigma: a d*d*k*nsample array
    #' - lpostdens: the log posterior density (vector of size nsample)
    #' - naccept! number of accepted proposals. 
{
    d <- ncol(x)
    output <- list(p = matrix(nrow=k, ncol=nsample),
                   Mu = array(dim = c(k, d, nsample)),
                   Sigma = array(dim = c(d, d ,k, nsample)),
                   lpostdens = rep(0, nsample),
                   naccept = 0
                   )
    current <- init
    current$lpost <- gmllk(x=x, Mu=current$Mu,
                            Sigma = current$Sigma, p=current$p) +
        dprior(Mu = current$Mu, Sigma = current$Sigma, p = current$p,
               hpar = hpar)
    ## lpost: logarithm of the unnormalized posterior density.
    
    for (niter in 1:nsample){
        proposal <- rproposal(Mu = current$Mu, Sigma = current$Sigma, p=current$p,
                              ppar = ppar)

        proposal$lpost <- gmllk(x=x, Mu=proposal$Mu,
                                Sigma = proposal$Sigma, p=proposal$p) +
            dprior(Mu = proposal$Mu, Sigma = proposal$Sigma, p = proposal$p,
                   hpar = hpar)
    
            
        llkmoveSigma <- sum(vapply(1:k, FUN = function(j){
            dwishart(Omega =proposal$Sigma[,,j], nu=ppar$nu_Sigma,
                     S = 1/ppar$nu_Sigma * current$Sigma[,,j] , log=TRUE)},
            FUN.VALUE = numeric(1)))

        llkbackSigma <- sum(vapply(1:k, FUN = function(j){
            dwishart(Omega =current$Sigma[,,j], nu=ppar$nu_Sigma,
                     S = 1/ppar$nu_Sigma * proposal$Sigma[,,j] , log=TRUE)},
            FUN.VALUE = numeric(1)))
        alphaPropmove <- sapply(ppar$alpha_p * current$p, function(x){max(x,1e-30)})
        alphaPropback <- sapply(ppar$alpha_p * proposal$p, function(x){max(x,1e-30)})
        #lacceptratio <-  proposal$lpost+llkmoveSigma*ddirichlet(proposal$p,alphaPropmove)-llkbackSigma*ddirichlet(proposal$p,alphaPropback)- current$lpost+
        lacceptratio <- min(proposal$lpost + ddirichlet(proposal$p,alphaPropmove,log = TRUE) - current$lpost - ddirichlet(current$p,alphaPropback,log = TRUE),0)
            ## Complete the code using
            ## lproposal$lpost,  current$lpost,
            ## ddirichlet( ... , log=TRUE), llkbackSigma and llkmovesigma. 

        U <- runif(1)
        if(U < exp(lacceptratio)){
            current <- proposal
            output$naccept <- output$naccept + 1
        }
        output$p[,niter] <- current$p
        output$Mu[,,niter] <- current$Mu
        output$Sigma[,,,niter] <- current$Sigma
        output$lpostdens[niter] <- current$lpost            
    }
    return(output)
    
} 
```
## 3 
```{r,warning=FALSE}
Kmc <- Kfit ## try with different values
init <- initPar(x=X, k=Kmc)

hpar <- list( alpha0= alpha0, 
           m0 = rep(0, d), beta0 = beta0, 
           W0 = W0, nu0 = nu0)

ppar <- list(var_Mu = 0.001,
            nu_Sigma = 500,
            alpha_p = 500) 


set.seed(1)
pct <- proc.time()
outputmh <- MHsample(x=X, k=Kmc, nsample= 3000,
                    init=init, hpar=hpar, ppar=ppar)
newpct <- proc.time()
elapsed <- newpct - pct
elapsed
outputmh$naccept 
```
## 4
```{r,warning=FALSE}
cdfTrace <- function(x , sample , burnin = 0 , thin = 1)
    #' Traces the evolution of the gmcdf at point x through the MCMC iterations.
    #'  Can be used for convergence monitoring. 
    #' x: a single point (vector of size d)
    #' burnin, thin: see MHpredictive
    #' returns: a vector of length [ (nsample - burnin )/thin ]
    {
    nsample <- ncol(sample$p)
    s<-burnin+1
    inds <- s:nsample
    inds <- inds[inds%%thin==0]
    output <- vapply(inds , function(niter){
      gmcdf(x , sample$Mu[,,niter] , sample$Sigma[,,,niter] ,sample$p[,niter])}, FUN.VALUE = numeric(1))
    return(output)}
```

## 5
```{r}
ix=X[floor(runif(1,0,dim(X)[1])),]
heidel.diag(mcmc(cdfTrace(ix,outputmh)))
```
Starting 1000 iterations, the test gives good results.
## 6
```{r, warning=FALSE}
init <- initPar(x=X, k=Kmc)
set.seed(1)
outputmh1 <- MHsample(x=X, k=Kmc, nsample= 3000,
                    init=init, hpar=hpar, ppar=ppar)
init <- initPar(x=X, k=Kmc)
set.seed(1)
outputmh2 <- MHsample(x=X, k=Kmc, nsample= 3000,
                    init=init, hpar=hpar, ppar=ppar)
init <- initPar(x=X, k=Kmc)
set.seed(1)
outputmh3 <- MHsample(x=X, k=Kmc, nsample= 3000,
                    init=init, hpar=hpar, ppar=ppar)
ix=X[floor(runif(1,0,dim(X)[1])),]
gelman.diag(mcmc.list(mcmc(cdfTrace(ix,outputmh1)),mcmc(cdfTrace(ix,outputmh2)),mcmc(cdfTrace(ix,outputmh3))))
gelman.plot(mcmc.list(mcmc(cdfTrace(ix,outputmh1)),mcmc(cdfTrace(ix,outputmh2)),mcmc(cdfTrace(ix,outputmh3))))
```
What is obvious is that there is no point in having more than 1500 iterations because the gain in "precision" is negligeable vs. the computational cost. However there seems to be a peek of gain in precision at around 750 iterations. Combining this with the previous test, we can say that 1000 to 1250 iterations are ideal for running this algorithm.
## 7
```{r}
MHpredictive <- function(x , sample , burnin=0, thin=1)
    #' posterior predictive density computed from MH output. 
    #' x: vector size d (single point)
    #' sample: output from the MCMC algorithm should contain
    #'    entries Mu, Sigma, p as in MHsample's output
    #' burnin: length of the burn-in period
    #'   (number of sample being discarded at the beginning of the chain).
    #' thin: thinning parameter: only 1 sample out of 'thin' will be kept
    #' returns: a single numeric value
{
    nsample <- ncol(sample$p)
    inds <- (burnin+1):nsample
    inds <- inds[inds%%thin==0]
    vectllk <- vapply(inds, function(niter){
      h=0
      for (i in 1:nrow(sample$p)){
        h=h+sample$p[i,niter]*dmnorm(x,sample$Mu[i,,niter],sample$Sigma[,,i,niter])
      }
      return(h)}
, FUN.VALUE = numeric(1)
  )
      return(mean(vectllk))
}
```


```{r,warning=FALSE}
xx <- seq(-2,2,length.out=20)
yy <- xx
dtrue <- outer(X= xx, Y=yy,
               FUN = function(x,y){
                 wrapper(x=x, y=y,
                         FUN=function(u,v){
                           exp(gmllk(x = c(u,v), Mu = Mu,
                                     Sigma = Sigma, p = p))
                         })
               })

dpredmh <-  outer(X= xx, Y=yy,
                  FUN = function(x,y){
                    wrapper(x = x, y = y,
                            FUN =function(u,v){
                              MHpredictive(x=c(u,v) , outputmh , burnin=0, thin=1)})
                  })

breaks <- c(seq(0.01,0.09, length.out=5),seq(0.1,0.3,length.out=5))
nbreaks <- length(breaks)
contour(xx,yy, z = dtrue, nlevels=nbreaks, levels = breaks)
contour(xx,yy, z = dpredmh,  nlevels=nbreaks, levels = breaks,
        add=TRUE, col='red')
```

The estimated density is very close to the true one. Each estimated countour is close to the true corresponding one. Thus, we are confident that the MH algorithm worked.


###4 - Predictive distributions versus maximum likelihood distributions

## 1
```{r,warning=FALSE}
MHpredictiveCdf <- function(x , sample , burnin = 0, thin = 1)
    #' posterior predictive cdf computed from MH output.
    #' arguments: see MHpredictive.
    #' returns: a single numeric value. 
{
    nsample <- ncol(sample$p)
    inds <- (burnin+1):nsample
    inds <- inds[inds%%thin==0]

    vectcdf <- vapply(inds, function(niter){
      gmcdf(x , sample$Mu[,,niter] , sample$Sigma[,,,niter] , sample$p[,niter])
  }, FUN.VALUE = numeric(1)
  )
      return(mean(vectcdf))
}
```
## 2 
```{r,warning=FALSE}
Pexcess <- rep(0,10)
Pexcess_vb <- Pexcess
Pexcess_mh <- Pexcess
thres_vect <-  seq(-1, 4, length.out=30)
for(i in seq_along(thres_vect)){
threshold <- rep(thres_vect[i], 2)
Pexcess[i] <- 1 - gmcdf(x = threshold, Mu = Mu, Sigma=Sigma, p=p)
Pexcess_vb[i] <-  1-vbPredictiveCdf(x=threshold,outputvb$Alphamat[,T],outputvb$Betamat[,T],outputvb$Marray[,,T],outputvb$Winvarray[,,,T], outputvb$Numat[,T])
Pexcess_mh[i] <-  1-MHpredictiveCdf(x=threshold,outputmh) 
}
ylim <- range(Pexcess, Pexcess_vb, Pexcess_mh)
plot(thres_vect,Pexcess, ylim = ylim)
lines(thres_vect, Pexcess_vb, col='red')
lines(thres_vect, Pexcess_mh, col='green')
```
The green curve is closer to the real one than the red one. This means that the CDF estimated via the MH algorithm yields better results than the CDF estimated with the VB algorithm.
## 3
```{r,warning=FALSE}
Pexcess <- rep(0,10)
Pexcess_vb <- Pexcess
Pexcess_mh <- Pexcess
thres_vect <- seq(1, 5, length.out=30)
for(i in seq_along(thres_vect)){
threshold <- rep(thres_vect[i], 2)
Pexcess[i] <- 1 - gmcdf(x = threshold, Mu = Mu, Sigma=Sigma, p=p)
Pexcess_vb[i] <-  1-vbPredictiveCdf(x=threshold,outputvb$Alphamat[,T],outputvb$Betamat[,T],outputvb$Marray[,,T],outputvb$Winvarray[,,,T], outputvb$Numat[,T])
Pexcess_mh[i] <-  1-MHpredictiveCdf(x=threshold,outputmh) 
}
ylim <- range(Pexcess, Pexcess_vb, Pexcess_mh)
plot(thres_vect,Pexcess, ylim = ylim)
lines(thres_vect, Pexcess_vb, col='red')
lines(thres_vect, Pexcess_mh, col='green')
```
The VB performs worse than the MH because it only gives an approximation even asymptotically.

